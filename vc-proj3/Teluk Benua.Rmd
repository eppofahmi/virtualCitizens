---
title: "Teluk Benua"
author: "Ujang Fahmi"
date: "4/9/2018"
output: 
  html_notebook:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Deskripsi
Menjajaki dan mencoba menemukan diskursus yang ada dibalik perbincangan di Twitter yang meyangkut kasus reklamasi Teluk Benua di Bali. Hal ini digunakan sebagai salah satu cara untuk memetakan publicness di seputar keberadaan change.org yang dapat digunakan oleh masyarakat untuk melakukan petisi. 

Kenapa mengambil kasus tentang teluk benua? Karena berdasarkan hasil pemetaan awal terhadap data Twitter yang memention akun @ChangeOrg_ID, salah satu kasus yang muncul adalah tentang Teluk Benoa, seperti dapat dilihat pada gambar di bawah ini. 

![hasil topic modelling twit mention to @changeOrg_ID](/Volumes/mydata/RStudio/virtualCitizens/vc-proj3/hasil-tm to change.png){width=70%}

Beberapa yang langkah ditempuh untuk dapat melakukan pemetaan nilai publicness di sekitar Teluk Benoa:

1. Mengambil data dari twitter dengan kata kunci teluk benoa (eksplorasi tagar, username)
2. Mengambil data tambahan tentang teluk benoa dengan tagar yang spesifik
3. Menggabungkan data hasil pencarian 
4. Membuat kolom keterangan duplicate atau tidak berdasarkan konten twit
5. Ambil twit yang tidak duplicate
6. Topic modelling
7. Network Analysis
8. Mensubset data berdasarkan parameter pengirim twit yang menjadi 10 username dengan betweeness tertinggi dalam network
9. Mengekspor data hasil subset per username untuk dibaca manual. 


# Library
```{r liball, message=FALSE, warning=FALSE,echo=TRUE}
library(twitteR)
library(dplyr)
library(tm)
library(stringr)
library(tidytext)
library(ggplot2)
library(lubridate)
library(tidyr)
library(tidyverse)
```

# Kata kunci Reklamasi
Langkah1 - mengambil dan mencoba memetakan data dengan menggunakan API dengan tujuan:

1. Mengetahui keragaman tagar yang digunakan
2. Mengetehui username yang terlibat

**setting api**
```{r api, include=FALSE}
#options(httr_oauth_cache=T)
#api_key <- "pAFA3zX08uixfVtP4PMpcHas0"
#api_secret <- "FZfuzn055vuJgRFraq8KNAAWipsa0ugUEpIjWxuhOFH9Kd5HAm"
#token <- "73705532-JApWQavtY5kkKbpRUGpDByEhHdLxb2HcKzAgtDZ7T"
#token_secret <- "2p2xuhlsMHVlbqDx8Swb7IkCAstwmCEMoINYreNmWxoCN"
#setup_twitter_oauth(api_key, api_secret, token, token_secret)
```

Proses pengambilan data dengan API - kata kunci yang digunakan adalah **reklamasi**. Harapannya, kata kunci tersebut dapat menangkap kasus tentang reklamasi, salah satunya yang terjadi di Bali. 

**Mengambil twit**
```{r}
#tb_api <- searchTwitter("reklamasi", n=30000, lang="id")
```

Dengan kata kunci **reklamasi** api dapat memeberikan twit sebanyak 5687

**Membuat data frame dan menyimpan data**
```{r}
#tb_api <- twListToDF(tb_api)
```

## tagar dari tb_api

**fungsi pengambilan tagar**
```{r}
# function detecting hashtag
tag_detect <- function(input_col) {
  hashtag <- str_extract_all(input_col, "#\\w+")
  # put tags in vector
  hashtag <- unlist(hashtag)
  # removing pic etc chr in the vector
  hashtag <- gsub("pic$", "", hashtag)
  hashtag <- gsub("http$", "", hashtag)
  hashtag <- gsub("https$", "", hashtag)
  hashtag <- gsub("XAmarthaXCoworkinc$", "", hashtag)
  hashtag <- tolower(hashtag)
  # calculate hashtag frequencies
  hashtag = table(hashtag)
  hashtag = as.data.frame(hashtag)
}
```

**pengambilan tagar**
```{r}
#tb_api_tagar <- tag_detect(tb_api)
```

```{r}
#tb_api_tagar %>%
#  arrange(desc(Freq)) %>%
#  slice(1:15) %>%
#  ungroup() %>%
#  mutate(word = factor(hashtag, unique(hashtag))) %>%
#  ungroup() %>%
#  ggplot(aes(x = reorder(hashtag, Freq), y = Freq)) + 
#  geom_col(show.legend = FALSE) +
#  coord_flip() +
#  labs(x = NULL, 
#       y = "15 Tagar paling sering digunakan dalam twit dengan kata kunci 'reklamasi'")
```

Dari data twit sebanyak 5687, terdapat 96 tagar. Tagar sepsifik yang "mungkin" berkaitan dengan bali adalah tagar **#balitolakreklamasi**. Namun twit dengan tagar ini tidak mungkin untuk diambil dengan api, karena di sini hanya digunakan sekali. Artinya, tidak banyak digunakan oleh warga net dalam twit dalam 7 hari terakhir. 

## Keputusan dan tindak lanjut
Kata kunci reklamasi tidak memberikan data. Jadi, data ini selanjutnya tidak akan digunakan lagi dalam analisis tentang publicness diseputar kasus **Teluk Benoa**. Dari sini, ada satu tagar yang perlu di jajaki yaitu **#balitolakreklamasi**. Namun dalam data ini tagar tersebut hanya digunakan satu kali. Oleh karena itu pencariaran twit akan dilakukan dengan metor **semi automatic scrapping**.  

# Tagar #balitolakreklamasi

**Catatan: scrapping dilkukan menggunakan python**
Now i have to stop here, and scrap some tweets using python........ 
Finally after few hours, I think we have the data now!!! Here is the plan to handle the data:

1. Impor data
2. Mendeteksi twit duplicate dengan menambah kolom logical 
3. Mendokumentasikan twit duplicate dalam data baru
4. Menghapus twit duplicate (rows)
5. Data wrangling (text cleaning, hastag extraction, term_count/row)
6. Ploting distribusi twit - group_by `date`
7. Next steps...

**raw_btr = data hasil scrapping dengan kata kunci tagar #balitolakreklamsi mentah**
```{r}
# memasukkan data hasil scrapping #balitolakreklamasi
raw_btr <- read.csv("btr2014-2018.csv", header = FALSE, stringsAsFactors = FALSE, sep = ";") 
colnames(raw_btr) <- c("date", "time", "user", "tweets", "replying", "rep_count", "ret_count", "fav_count","link")
# converting date format
raw_btr$date <- as.Date(raw_btr$date,format='%d %b %Y')
```

Twit hasil scrapping dari tahun 2013 - 2018 sebanyak 72641

## Mendeteksi twit duplicate

Kode di bawah ini digunakan untuk membuat sebuah kolom baru yang bernama `is_duplicate` yang berisi informasi logical yang menerangkan apakah sebuah twit memiliki duplicate atau tidak berdasarkan kesamaan konten yang ada dalam kolom `tweets`. 

```{r}
# make a new logical column based on tweets column 
raw_btr <- raw_btr %>%
  dplyr::mutate(is_duplicate = duplicated(tweets))
```

Selanjutnya, saya ingin membuat sebuah data baru yang hanya berisi twit duplicate. 
```{r}
btr_dup <- raw_btr %>%
  filter(is_duplicate == TRUE)
```

Berikut adalah contoh isi twit duplicate.

```{r}
head(btr_dup$tweets)
```

**Catatan: isi twit no 1 bukan duplicate no 2 dan sebaliknya, tapi isi twit tersebut menduplicate isi twit lain yang sudah ada lebih dulu berdasarkan waktu posting**.

Twit duplicate ada 12922 twit. Berikutnya, saya membuat data baru, dengan subset data yang `is_duplicate` nya == FALSE

```{r}
btr_no_dup <- raw_btr %>%
  filter(is_duplicate == FALSE)
```

59719 twit akan diproses pada tahapan selanjutnya. 

## Data wrangling 

(text cleaning, hastag extraction, term_count/row)

### Cleaning tweets 1
```{r}
# fungsi untuk cleaning 
tweet_cleaner1 <- function(input_text) # nama kolom yang akan dibersihkan
{    
  # create a corpus (type of object expected by tm) and document term matrix
  corpusku <- Corpus(VectorSource(input_text)) # make a corpus object
  # remove urls1
  removeURL1 <- function(x) gsub("http[^[:space:]]*", "", x) 
  corpusku <- tm_map(corpusku, content_transformer(removeURL1))
  #remove urls3
  removeURL2 <- function(x) gsub("pic[^[:space:]]*", "", x) 
  corpusku <- tm_map(corpusku, content_transformer(removeURL2))
  #remove puntuation
  removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]@#_]*", "", x) # kecuali @ dan #
  corpusku <- tm_map(corpusku, content_transformer(removeNumPunct))
  
  corpusku <- tm_map(corpusku, stripWhitespace)
  #kata khusus yang dihapus
  corpusku <- tm_map(corpusku, removeWords, c("iki"))
  corpusku <- tm_map(corpusku, stripWhitespace)
  #removing white space in the begining
  rem_spc_front <- function(x) gsub("^[[:space:]]+", "", x)
  corpusku <- tm_map(corpusku, content_transformer(rem_spc_front))
  
  #removing white space at the end
  rem_spc_back <- function(x) gsub("[[:space:]]+$", "", x)
  corpusku <- tm_map(corpusku, content_transformer(rem_spc_back))
  
  data <- data.frame(text=sapply(corpusku, identity),stringsAsFactors=F)
  
  a <- bind_cols(btr_no_dup, data)
}
```

**clean text**
```{r}
btr_no_dup <- tweet_cleaner1(btr_no_dup$tweets)
```

### Menampbahkan daftar username dan menghitung jumlah per row
```{r}
btr_no_dup$user_all <- sapply(str_extract_all(btr_no_dup$text, "@\\S+", simplify = FALSE), paste, collapse=", ")

# add @ if nedeed
#user_change$User <- paste("@", user_change$User, sep="")

# merge column user and user_all
btr_no_dup$user_all <- paste(btr_no_dup$user, btr_no_dup$user_all, sep=" ")
#str(rawTweet_change)

btr_no_dup$user_count <- sapply(btr_no_dup$user_all, 
                              function(x) length(unlist(strsplit(as.character(x), "@\\S+"))))
```

Catatan: Hasilnya masih belum bersih

### Extract hashtag
```{r}
btr_no_dup$hashtag <- sapply(str_extract_all(btr_no_dup$text, "#\\S+", simplify = FALSE), paste, collapse=", ")

btr_no_dup$tag_count <- sapply(btr_no_dup$hashtag, 
                                     function(x) length(unlist(strsplit(as.character(x), "#\\S+"))))
```

**menyusun ulang data**
```{r}
names(btr_no_dup)

# i need to restructuring the data
btr_no_dup <- btr_no_dup[ , c("date", "user", "tweets","is_duplicate", "replying", "rep_count", "ret_count", "fav_count", "user_all", "user_count", "hashtag", "tag_count", "link")]
```

### Cleaning tweets 2

```{r}
# fungsi untuk cleaning 
tweet_cleaner2 <- function(input_text) # nama kolom yang akan dibersihkan
{    
  # create a corpus (type of object expected by tm) and document term matrix
  corpusku <- Corpus(VectorSource(input_text)) # make a corpus object
  # remove urls1
  removeURL1 <- function(x) gsub("http[^[:space:]]*", "", x) 
  corpusku <- tm_map(corpusku, content_transformer(removeURL1))
  #remove urls3
  removeURL2 <- function(x) gsub("pic[^[:space:]]*", "", x) 
  corpusku <- tm_map(corpusku, content_transformer(removeURL2))
  #remove username 
  TrimUsers <- function(x) {
    str_replace_all(x, '(@[[:alnum:]_]*)', '')
  }
  corpusku <- tm_map(corpusku, TrimUsers)
  #remove all "#Hashtag1"
  removehashtag <- function(x) gsub("#\\w+", "", x)
  corpusku <- tm_map(corpusku, content_transformer(removehashtag))
  #merenggangkan tanda baca
  tandabaca1 <- function(x) gsub("((?:\b| )?([.,:;!?()]+)(?: |\b)?)", " \\1 ", x, perl=T)
  corpusku <- tm_map(corpusku, content_transformer(tandabaca1))
  #remove puntuation
  removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x) # kecuali - dan '
  corpusku <- tm_map(corpusku, content_transformer(removeNumPunct))
  corpusku <- tm_map(corpusku, stripWhitespace)
  corpusku <- tm_map(corpusku, content_transformer(tolower)) 
  #stopwords bahasa indonesia
  stopwords <- read.csv("stopwords_indo.csv", header = FALSE)
  stopwords <- as.character(stopwords$V1)
  stopwords <- c(stopwords, stopwords())
  corpusku <- tm_map(corpusku, removeWords, stopwords)
  #kata khusus yang dihapus
  corpusku <- tm_map(corpusku, removeWords, c("rt", "cc", "via", "jrx", "balitolakreklamasi", "acehjakartajambisurabayabalintbpaluambon"))
  corpusku <- tm_map(corpusku, stripWhitespace)
  #removing white space in the begining
  rem_spc_front <- function(x) gsub("^[[:space:]]+", "", x)
  corpusku <- tm_map(corpusku, content_transformer(rem_spc_front))

  #removing white space at the end
  rem_spc_back <- function(x) gsub("[[:space:]]+$", "", x)
  corpusku <- tm_map(corpusku, content_transformer(rem_spc_back))
  data <- data.frame(clean_text=sapply(corpusku, identity),stringsAsFactors=F)
}
```

Fungsi di atas, akan diulang-ulang sampai text dirasa sudah bersih.

Menambahkan dua kolom, `clean_text` dan `word_count` yang akan digunakan untuk eksplrasi lebih jauh dalam text mining. 

```{r}
# eksekusi fungsi di atas
btr_no_dup_clean <- tweet_cleaner2(btr_no_dup$tweets)
btr_no_dup_clean$word_count <- sapply(btr_no_dup_clean$clean_text, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
```

Selanjutnya, saya akan membuat daftar kata yang ada dalam text, untuk dijadikan pedoman dalam cleaning text selanjutnya. 

**tdm creator**
```{r}
tdm_creator <- function(input_text)
{    
  Corpus <- VCorpus(VectorSource(input_text))
  tdm <- TermDocumentMatrix(Corpus, control=list(wordLengths = c(1, Inf)))
  unique_indexes <- unique(tdm$i)
  tdm <- tdm[unique_indexes,]
}

# test 
tdm <- tdm_creator(btr_no_dup_clean$clean_text)
```

**count term**
```{r}
# fungsi untuk menghitung frequensi kata
count_term <- function(tdm_input)
{    
  #(freq.terms <- findFreqTerms(dtm_input, lowfreq = 100))
  term.freq <- rowSums(as.matrix(tdm_input))
  term.freq <- subset(term.freq, term.freq >= 1) # frekuensi minimal kata
  df <- data.frame(term = names(term.freq), freq = term.freq)
}

frekuensi_kata <- count_term(tdm)
```

**top freq words**
```{r}
library(tidyverse)
frekuensi_kata <- frekuensi_kata %>%
  arrange(desc(freq))

write_csv(frekuensi_kata, path = "teluk benoa output/term freq-teluk benoa.csv")
```

**20 kata paling sering digunakan**
```{r}
frekuensi_kata %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = factor(term, unique(term))) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(term, freq), y = freq)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "Top 30 term in the tweets")
```

**tf-idf**
```{r}
# fungsi untuk menghitungan tf-idf
tfidf_count <- function(tdm_input)
{
  tidy_all <- tidy(tdm_input)
  tf_idf_all <- tidy_all %>%
    bind_tf_idf(term, document, count)
}

#menggunakan fungsi
tweets_tf_idf <- tfidf_count(tdm)
write_csv(tweets_tf_idf, path = "teluk benoa output/tfidf-teluk benoa.csv")
```

**top tifidf**
```{r}
tweets_tf_idf <- tweets_tf_idf %>%
  arrange(desc(tf_idf))
tweets_tf_idf
```

**Catatan:** sebaiknya sebelum melakukan penilaian tf-idf dokumen dibagi terlebih dahulu kedalam beberapa kluster, misalnya berdasarkan tahun, atau rentang bulan (e.g: per 6 bulan). Sehingga urgensi kata bisa di nilai pada tiap kluster, tidak seperti di atas, per dokumen/row. 

Dengan dua data yang baru saja dibuat, selanjutnya perlu diputuskan term mana saja yang akan dihapus. Jika sudah, maka kembali lagi ke cleaning 2. 

### Menggabungkan hasil cleaning 2

Di sini kita akan menggbaungkan hasil cleaning 2, dengan data lama, dengan fungsi `bind_cols`, untuk menggabungkan kolom. 

```{r}
tweets_btr <- bind_cols(btr_no_dup, btr_no_dup_clean)
write_csv(tweets_btr, path = "teluk benoa output/btr_tweets_ready.csv")
```

File di atas, selanjutnya bisa dijadikan sebagai input untuk topic modelling karena twitnya sudah dibersihkan. 


# Eksplorasi 1 - Distribution 

```{r}
btr_dist <- tweets_btr[, c("date", "fav_count", "rep_count", "ret_count", "user_count")]
btr_dist$ret_count <- as.integer(btr_dist$ret_count)
```


```{r}
btr_dist <- tidyr::replace_na(btr_dist, list(fav_count=0, rep_count=0, ret_count=0))

# counting tweets distribution
df1 <- btr_dist %>%
  group_by(date) %>%
  count(date)

# counting users and activities distribution
df2 <- btr_dist %>% 
  group_by(date) %>% 
  summarise_all(sum)

# combining two data frame
btr_dist <- as.data.frame(bind_cols(df1, df2))
btr_dist <- btr_dist %>%
  select(date, fav_count, rep_count, ret_count, user_count, tweet_count = n)
```

**Ploting user activities**
```{r}
library(reshape2)
library(scales)

# melt the data for ggplot using `reshape2`
datanya_long <- melt(btr_dist, id = "date")

# ploting to goem_line ggplot2
plot_interaksi <- ggplot(data=datanya_long,
                aes(x=date, y=value, colour=variable)) +
  geom_line() +
  scale_x_date(labels = date_format("%Y-%m"), 
               breaks = date_breaks("12 months")) +
  facet_grid(variable~., scales="free") +
  theme(legend.position="top")

plot_interaksi
```

Dari plot dapat dilihat bahwa tagar #balitolakreklamasi paling banyak digunakan pada pertengahan tahun 2014. Lebih spesifiknya pada 2014-06-14 terdapat 2312, dan pada 2014-06-16 ada 1194 twit, sementara rata-rata perharinya sekitar 460. Hal ini juga cenderung diikuti oleh jumlah lainnya, seperti favorite, reply, retweet dan username yang terlibat. 

**Pertanyaan:** apa yang menyebabkan pada tanggal itu pengguna twitter banyak menggunakan tagar? dan apa isi twitnnya. 

Untuk menjawab pertanyaan tersebut, saya mengambil twit dari tanggal di mana orang banyak menggunakan tagar. 

```{r}
# melihat tanggal twit yang memiliki rerata lebih dari 460
btr_dist %>%
  filter(tweet_count > 460) %>%
  ggplot(aes(x = as.character(date), y = tweet_count)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "Tanggal ketika tagar lebih sering digunakan")
#tweets_btr
```
Tepatnya ada 14 hari di mana pengguna twitter lebih sering menggunakan tagar ini dibanding hari lainnya. Data ini akan saya beri nama `btr_anomali`. 

2014-06-14 to 2014-06-17
2014-06-19
2014-06-23
2014-06-26
2014-08-23
2014-08-25 to 2014-08-27
2014-08-29 to 2014-08-30
2014-09-30

**Pengambilan data 14 hari**
```{r}
btr_anomali <- tweets_btr %>%
  subset(date >= "2014-06-14" & date <= "2014-06-17" | date == "2014-06-19" | date == "2014-06-23" | date == "2014-06-26" | date == "2014-08-23" | date >= "2014-08-25" & date <= "2014-08-27" | date >= "2014-08-29" & date <= "2014-08-30" | date == "2014-09-30")

write_csv(btr_anomali, path = "teluk benoa output/btr-anomali.csv")
```

Dalam 14 hari tersebut di atas, terdapat 11383 twit yang menggunakan tagar atau 19.06 persen dari kesuluruhan data. 

**Catatan:** untuk mendalami isu, mungkun close reading terhadap data ini perlu dilakukan. 

# Eksplorasi 2 - Tagar
**Tujuan:** untuk mengetahui tagar apa saja yang digunakan dalam twit

```{r}
# function detecting hashtag
tag_detect <- function(input_col) {
  hashtag <- str_extract_all(input_col, "#\\w+")
  # put tags in vector
  hashtag <- unlist(hashtag)
  # removing pic etc chr in the vector
  hashtag <- gsub("pic$", "", hashtag)
  hashtag <- gsub("http$", "", hashtag)
  hashtag <- gsub("https$", "", hashtag)
  hashtag <- tolower(hashtag)
  # calculate hashtag frequencies
  hashtag = table(hashtag)
  hashtag = as.data.frame(hashtag)
} 
# run the funtion 
btr_tagar <- tag_detect(tweets_btr$tweets)
```

**Visualisasi 20 tagar paling sering digunakan**
```{r}
btr_tagar %>%
  filter(!str_detect(hashtag, '#balitolakreklamasi')) %>%
  arrange(desc(Freq)) %>%
  top_n(20) %>%
  ggplot(aes(x = reorder(hashtag, Freq), y = Freq)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "20 Tagar paling sering digunakan dalam twit")
```

# Eksplorasi 3 - Username

```{r}
# pengirim
user_pengirim <- tweets_btr %>%
  unnest_tokens(user, user, to_lower = FALSE) %>%
  count(user, sort = TRUE) %>%
  arrange(desc(n)) %>%
  ungroup()
# terlibat 
user_terlibat <- tweets_btr %>%
  unnest_tokens(user, user_all, to_lower = FALSE) %>%
  count(user, sort = TRUE) %>%
  arrange(desc(n)) %>%
  ungroup()
```

**Visualisasi user pengirim:**

```{r}
user_pengirim %>%
  arrange(desc(n)) %>%
  top_n(20) %>%
  ggplot(aes(x = reorder(user, n), y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "20 Username paling sering mengirim twit")
```

**Visualisasi user terlibat:**

```{r}
user_terlibat %>%
  arrange(desc(n)) %>%
  top_n(20) %>%
  ggplot(aes(x = reorder(user, n), y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "20 Username paling sering terlibat dalam twit")
```


# Input 1 - Network analysis 
```{r}
input_net_btr <- tweets_btr %>%
  select(Data = user_all)
input_net_btr$Data <- gsub("[^[:alpha:][:space:]@_]*", "", input_net_btr$Data)

write_csv(input_net_btr, path = "teluk benoa output/net_benua.csv")
```


# input 2 - Topic Modelling
untuk topic modelling kita hanya butuh 3 kolom, yaitu `date`, `clean_text`, dan `word_count`. Kolom `date` digunakan untuk melakukan pelacakan perkembangan topic. `clean_text` digunakan sebagai input data, dan `word_count` digunakan untuk memfilter rows yang kosong atau tidak memiliki term setelah proses cleaning. 

```{r}
# memilih kolom untuk data
input_tm <- tweets_btr[,c("date", "clean_text","word_count")]
input_tm <- input_tm %>%
  filter(word_count > 1)
#menyimpannya
write_csv(input_tm, path = "teluk benoa output/tm_benua.csv")
```

**Keterangan:** file yang disimpan untuk input tm terdiri dari rows yang di dalamnya terdapat minimal 2 kata. Hal ini dilakukan karena topic modelling akan menggunakan bigram. Dengan kata lain, topic modelling dilakukan terhadap term yang memiliki konteks lokal, atau term lain yang bisa menambah penjelasan dari sebuah term. 


# Rankuman 
Sampai di sini kita sudah melakukan eksplorasi beberapa hal:

1. Parameter pencarian data - **pertama**, parameter yang digunakan adalah twit yang memention akun Twitter @changeOrg_ID. Dari data yang didapatkan kita mendapatkan beberapa topik, salah satunya adalah tentang **teluk benoa**. **Kedua**, karena teluk benoa merupakan kasus tentang reklamasi kita mencoba kata kunci **reklamasi** dengan API Twitter. Dari data yang didapat kita mendapatkan kata kunci yang berkaitan, yaitu tagar **#balitolakreklamasi**. **Ketiga** tagar tersebut kemudian dijadikan parameter pencarian berikutnya dan mendapatkan data sebanyak **72.641** twit. Setelah melalui pengecekan duplikasi, data yang dapat di analisis sebanyak 59.719 twit.

2. Dari 59.719 selanjutnya dilihat distribusinya. Beradasrkan visualiasi distribusi diketahui bahwa pertengahan tahun 2014 merupakan puncak penggunaan tagar. Untuk itu, saya sediakan data dari tanggal di mana tagar digunakan lebih dari rerata secara umum. Hasilnya, terdapat 14 hari, di mana tagar lebih sering digunakan. Untuk kajian lebih lanjut dan mendalam, saya subset data dari tanggal-tanggal tersebut untuk **close reading** dengan nama data `btr_anomali`. Data tersebut berisi 11382 twit dengan 15 kolom/variabel, termasuk raw twit dan twit yang sudah dibersihkan dari elemen-elemen lainya. 

3. Berdasarkan pemetaan tagar, diketahui bahwa selain tagar #balitolakreklamasi terdapat tagar lain yang sering digunakan. Salah satunya adalah tagar #batalkanperpres51th2014. Sehingga untuk kajian lebih mendalam pada langkah selanjutnya akan lebih baik jika melakukan studi terhadap *cases* yang ada di seputar teluk benoa ini. 

4. Untuk tahap selanjutnya, proses dalam script ini menyediakan beberapa output data, yang perlu dilihat secara manual untuk memastikan proses cleaning. Data tersebut adalah: (1) `term freq-teluk benoa.csv`; dan (2) `tfidf-teluk benoa.csv`. Dua data tersebut digunakan untuk melihat term apa saja yang tidak diperlukan dan bisa di delete dari data untuk memperkecil jumlah term, dan meringkan proses komputasi. **Topik modelling dan network analisis bisa dilakukan setelah memastikan data cukup bersih.** 

5. Untuk tf-idf jika memungkinkan ada timeline kasus teluk benua, sehingga kita bisa lakukan clustering data terlebih dahulu. Misalnya: data dibagi dalam beberapa fase yang menunjukkan perkembangan kasus. Fase pertama: perencanaan reklamasi dari tanggal sekian sampai sekian. Fase kedua: munculnya perpres. Fase ketiga, empat dan seterusnya. 

Demikian laporan ini saya buat, dan **happy close reading**.

**Catatan:** 

1. Semua data ini dapat dilihat [di sini](https://drive.google.com/drive/folders/1wO6bqd5adJkISwIb7bYJ89P1gJW9-4gh?usp=sharing)
2. Untuk update bisa cek [di sini](https://github.com/eppofahmi/virtualCitizens)

Berikut ini adalah twit yang diunggah oleh 5 username yang memiliki betweenes tertinggi. 
```{r}
jrx <- raw_btr %>%
  filter(str_detect(user, '@JRX_SID'))
gendovara <- raw_btr %>%
  filter(str_detect(user, '@gendovara'))
```

